# üîß BIOSage 2.0: Smart BIOS Companion with Integrated Local LLM

Welcome to **BIOSage 2.0**, a next-generation, intelligent BIOS-style web interface designed to replace outdated, static BIOS UIs with an AI-powered system insight platform. Featuring real-time diagnostics, human-readable explanations, and multilingual support ‚Äî all while running offline or over LAN.

## üöÄ Project Highlights

- ‚úÖ Reimagined BIOS Interface with terminal-inspired UI
- üß† Integrated LLM (LLaMA 3.2B via Ollama) for smart responses and diagnostics
- üåê Language translation for LLM outputs: **Hindi**, **English**, **Russian**
- üì¶ Full system diagnostics via `systeminformation` module
- üîå Seamless backend-to-AI integration using local EC2 instance
- üìä Real-time monitoring of CPU, RAM, storage, network, temps, and more
- üîí Works offline (LAN mode) with private EC2-hosted LLM ‚Äî No internet needed

---

## üßë‚Äçüíª Tech Stack

| Layer        | Tech Used                                 |
|--------------|--------------------------------------------|
| **Frontend** | Next.js, Tailwind CSS, React, TypeScript   |
| **Backend**  | Node.js, Express, systeminformation        |
| **LLM Engine** | [Ollama](https://ollama.com) running LLaMA 3.2B |
| **Hosting**  | Vercel (frontend), EC2 (backend LLM instance) |
| **Translation** | Google Translate API / Custom Dictionary Map |

---

## üì∏ Screenshots

### BIOSage Landing Page
![system-stats](https://github.com/avyuktsoni0731/BIOSage/blob/f6e62f2865992dd27cd9115eb9ee4d314bf10b3d/public/landing.png)

### Error Response
![error-response](https://github.com/avyuktsoni0731/BIOSage/blob/f6e62f2865992dd27cd9115eb9ee4d314bf10b3d/public/response.png)

### EC2 Ollama Response
![ollama-response](https://github.com/avyuktsoni0731/BIOSage/blob/f6e62f2865992dd27cd9115eb9ee4d314bf10b3d/public/ollama-response.png)

### Hardware Monitor
![hardware-monitor](https://github.com/avyuktsoni0731/BIOSage/blob/f6e62f2865992dd27cd9115eb9ee4d314bf10b3d/public/hardware-monitor.png)

### Advanced Features
![advanced-features](https://github.com/avyuktsoni0731/BIOSage/blob/f6e62f2865992dd27cd9115eb9ee4d314bf10b3d/public/advanced-features.png)

---

## üõ† Features Breakdown

### ‚öôÔ∏è Smart System Insights
- Uses [`systeminformation`](https://www.npmjs.com/package/systeminformation) to fetch:
  - CPU, RAM, Drives
  - Battery status, network stats
  - BIOS info, motherboard, and thermal sensors

### üß† Local LLM Support
- Prompts are sent to a locally running Ollama server (on Amazon EC2).
- Model used: `llama3:3b-instruct` ‚Äî optimized for fast, high-quality local inference.
- No internet dependency required in BIOS ‚Äî runs entirely on LAN.

### üåç Language Translation
- LLM responses are dynamically translated into:
  - **üáÆüá≥ Hindi**
  - **üá∫üá∏ English**
  - **üá∑üá∫ Russian**
- Choose your preferred language for BIOS suggestions and error explanations.

### üìâ Real-time Telemetry
- Frontend displays live data for:
  - Fan RPM
  - CPU temperature
  - System uptime
  - Memory usage and load averages

---

## üìÅ Project Structure

```

bios-2.0/
‚îú‚îÄ‚îÄ frontend/                # Next.js frontend
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ api/llm-generate     # LLM interaction (hosted on EC2)
‚îî‚îÄ‚îÄ README.md

```

---

## ‚ö†Ô∏è Deployment Considerations

### ‚úÖ Works Locally
- On local systems (e.g. inside a BIOS-like Linux shell), `systeminformation` works flawlessly.
- Ollama requests are sent to a LAN-accessible IP (your EC2 instance), with low latency.

### üö´ Vercel Limitations
- If deployed on **Vercel**, direct hardware access is **not** possible due to:
  - Container isolation
  - No access to system `/dev/` or hardware APIs
- But that‚Äôs fine ‚Äî BIOS runs on the local machine, and this is just the **UI demo**.

> üîê This doesn‚Äôt affect production use ‚Äî because BIOS is inherently local.  
> On local installs (or embedded firmware), everything works as intended.

---

## üí¨ Sample Usage (Terminal-Style Prompts)

```

> diagnose-cpu
> LLM: Your CPU is currently operating under high thermal load. Consider increasing airflow or underclocking.

> why-is-my-pc-shutting-down
> LLM: Sudden shutdowns are often caused by power supply instability or overheating...

> ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§ì
> LLM (Hindi): ‡§Ü‡§™‡§ï‡•á CPU ‡§ï‡§æ ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡•à‡§® ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§ú‡§æ‡§Ç‡§ö‡•á‡§Ç ‡§Ø‡§æ ‡§π‡•Ä‡§ü‡§∏‡§ø‡§Ç‡§ï ‡§≤‡§ó‡§æ‡§è‡§Ç‡•§

> –ø–æ-—Ä—É—Å—Å–∫–∏
> LLM (Russian): –í–∞—à –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–µ—Ä–µ–≥—Ä–µ–≤–∞–µ—Ç—Å—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∏—Å—Ç–µ–º—É –æ—Ö–ª–∞–∂–¥–µ–Ω–∏—è.

````

---

## üåê API Endpoints

| Route                  | Purpose                             |
|------------------------|-------------------------------------|
| `GET /api/system-info`     | Fetches real-time system data       |
| `POST /api/generate`     | Sends a prompt to the LLM server    |
| `POST /api/generate`  | Translates response to chosen lang  |

---

## üß™ How to Run

1. Clone the repo:
   ```bash
   git clone https://github.com/avyuktsoni0731/biosage.git

2. Install frontend:

   ```bash
   npm install && npm run dev

3. Run Ollama (on EC2):
   - Create a `.env` file with `NEXT_PUBLIC_EC2_IP` environment variable with `<public_ec2_ip>` variable value (all thanks to `@souvlakee` for providing us w/ the EC2 instance).
   - PS: he knows the IP (if you guys wanna test it out :D)

---

## üìπ Demo Video

> https://youtu.be/poh1ABrmZHw
